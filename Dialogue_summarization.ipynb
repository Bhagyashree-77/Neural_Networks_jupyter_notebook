{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f075838a-754d-472e-b378-eee75a42b061",
   "metadata": {},
   "source": [
    "# Set up kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc58be36-6bbf-4c5f-b2c9-b42eb0e4a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\bhagyashree\\anaconda3\\lib\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/8a/6a/19e9fe04fca059ccf770861c7d5721ab4c2aebc539889e97c7977528a53b/pip-24.0-py3-none-any.whl.metadata\n",
      "  Using cached pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-24.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '%pip'\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 \\\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score=0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b188137-de3e-4d7d-8ffe-8dabfe318fe0",
   "metadata": {},
   "source": [
    "# Import necessary components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0af021-c332-495b-9686-f3e39f2a6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer,GenerationConfig,TrainingArguments,Trainer\n",
    "import torch\n",
    "import time \n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab656f4-359a-4c4c-bc74-64e610d6e8e5",
   "metadata": {},
   "source": [
    "# Load Dataset and LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d93d6aa-1654-4caf-91a9-7452e2e37e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name=\"knkarthick/dialogsum\"\n",
    "dataset=load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e897ebf5-8424-40ea-9b73-01e5b4a1f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "original_model=AutoModelForSeq2SeqLM.from_pretrained(model_name,torch_dtype=torch.bfloat16)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa21345-9036-44c8-b0b6-307012e07df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_model_parameters:247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters:100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params=0\n",
    "    all_model_params=0\n",
    "    for _,param in model.named_parameters():\n",
    "        all_model_params+=param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params+=param.numel()\n",
    "        percentage_trainable_params = (trainable_model_params / all_model_params) * 100\n",
    "    return f\"trainable_model_parameters:{trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters:{percentage_trainable_params:.2f}%\"\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8f638-4577-4e73-b763-1bdce3a7ea01",
   "metadata": {},
   "source": [
    "# Test the Model with zero shot inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6d3ed30-b7e8-4019-9041-85840536b232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\"\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index =200\n",
    "\n",
    "dialogue=dataset['test'][index]['dialogue']\n",
    "summary=dataset['test'][index]['summary']\n",
    "\n",
    "prompt=f\"\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "summary:\n",
    "\"\"\"\n",
    "inputs=tokenizer(prompt,return_tensors='pt')\n",
    "output=tokenizer.decode(original_model.generate(inputs[\"input_ids\"],max_new_tokens=200,)[0],skip_special_tokens=True)\n",
    "dash_line='-'.join(''for x in range (100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302609d-3a8b-4a9e-8895-db7c7d0ef2e5",
   "metadata": {},
   "source": [
    "# PERFORM FULL FINE TUNING ## you need to convert dialog-summary pairs into explicit instructions for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e37c0bd-4212-4dad-ad6e-d887412f6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt='Summarize the following conversation.\\n\\n'\n",
    "    end_prompt='\\n\\nSummary'\n",
    "    prompt=[start_prompt+dialogue+end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids']=tokenizer(prompt,padding=\"max_length\",truncation=True,return_tensors=\"pt\").input_ids\n",
    "    example['labels']=tokenizer(example[\"summary\"],padding=\"max_length\",truncation=True,return_tensors=\"pt\").input_ids\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train ,validation,test.\n",
    "# the tokenize_function code is handling all data across all splits in batches \n",
    "tokenized_datasets=dataset.map(tokenize_function,batched=True)\n",
    "tokenized_datasets=tokenized_datasets.remove_columns(['id','topic','dialogue','summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0622a7d6-03c3-4f45-8f17-cb9053e71dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets=tokenized_datasets.filter(lambda example,index:index%100==0,with_indices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3daacbcf-665b-48ac-b765-706389198aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the dataset:\n",
      "Training:(125, 2)\n",
      "Validation:(5, 2)\n",
      "Test:(15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes of the dataset:\")\n",
    "print(f\"Training:{tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation:{tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test:{tokenized_datasets['test'].shape}\")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f93214-faee-4d93-a0b9-532d35576fc6",
   "metadata": {},
   "source": [
    "# Fine Tune The model with the preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78da74ad-64ad-483c-bef5-7a95cf91a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utilize the built in Hugging face Trainer class.Pass the preprocessed dataset with ref to the original model\n",
    "output_dir=f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "trainer=Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631de1d7-dccf-469c-adb8-ff54f3d85a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhagyashreetikhe52\u001b[0m (\u001b[33martificial_intelligence\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sagemaker-user/wandb/run-20240128_094551-ueldk850</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/artificial_intelligence/huggingface/runs/ueldk850' target=\"_blank\">stilted-bush-3</a></strong> to <a href='https://wandb.ai/artificial_intelligence/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/artificial_intelligence/huggingface' target=\"_blank\">https://wandb.ai/artificial_intelligence/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/artificial_intelligence/huggingface/runs/ueldk850' target=\"_blank\">https://wandb.ai/artificial_intelligence/huggingface/runs/ueldk850</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3435820-184f-4360-b408-2997f771c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/generation_config.json to flan-dialogue-summary-checkpoint/generation_config.json\n",
      "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/trainer_state.json to flan-dialogue-summary-checkpoint/trainer_state.json\n",
      "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/rng_state.pth to flan-dialogue-summary-checkpoint/rng_state.pth\n",
      "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/config.json to flan-dialogue-summary-checkpoint/config.json\n",
      "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/scheduler.pt to flan-dialogue-summary-checkpoint/scheduler.pt\n",
      "download: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/training_args.bin to flan-dialogue-summary-checkpoint/training_args.bin\n",
      "download failed: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/pytorch_model.bin to flan-dialogue-summary-checkpoint/pytorch_model.bin [Errno 28] No space left on device\n",
      "download failed: s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/optimizer.pt to flan-dialogue-summary-checkpoint/optimizer.pt [Errno 28] No space left on device\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53002318-df6e-49d8-920e-28f86af72afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 sagemaker-user users 945M May 15  2023 ./flan-dialogue-summary-checkpoint/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./flan-dialogue-summary-checkpoint/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0d489ee-28f3-43f2-b103-4ab26f9e0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model=AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\",torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5ba78-ee3b-49d4-bfc6-b16625274693",
   "metadata": {},
   "source": [
    "# EVALUATE THE MODEL QUALITATIVELY(HUMAN EVALUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6327d45b-f044-4b29-84ee-c9dfd4dd3196",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'instruct_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m original_model_outputs\u001b[38;5;241m=\u001b[39moriginal_model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids,generation_config\u001b[38;5;241m=\u001b[39mGenerationConfig(max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m original_model_text_output\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(original_model_outputs[\u001b[38;5;241m0\u001b[39m],skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m instruct_model_outputs\u001b[38;5;241m=\u001b[39m\u001b[43minstruct_model\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids,generation_config\u001b[38;5;241m=\u001b[39mGenerationConfig(max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m instruct_model_text_output\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(instruct_model_outputs[\u001b[38;5;241m0\u001b[39m],skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m dash_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m100\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'instruct_model' is not defined"
     ]
    }
   ],
   "source": [
    "index=200\n",
    "dialogue=dataset['test'][index]['dialogue']\n",
    "human_baseline_summary=dataset['test'][index]['summary']\n",
    "prompt=f\"\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "input_ids=tokenizer(prompt,return_tensors='pt').input_ids\n",
    "\n",
    "original_model_outputs=original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "original_model_text_output=tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs=instruct_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "instruct_model_text_output=tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "dash_line='-'.join(''for x in range (100))\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:/n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c50604-8149-47b2-8e27-c1cdc9025502",
   "metadata": {},
   "source": [
    "# Evaluate the model Quantitavely(ROUGE METRIC) \n",
    "### The ROUGE metric helps the quantify the validity of summarization produced by models.it compares summarization to a \"baseline\"summary \n",
    "### which is created by human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b0582d-2209-4c2a-a94a-e802aaa0e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (2.0.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=19da958003eb242d0999635ec7ae083e804b23df76fa71112219ba78a87f6904\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score\n",
    "rouge=evaluate.load('rouge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57fee8f4-044b-4cf5-99ef-8d62b5b25be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge=evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcaa8d91-ea89-49e8-8026-3b1bebbc0bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Then, you'll have to go to the hospital.</td>\n",
       "      <td>Then, the man asks the woman to tell him the n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "\n",
       "                   original_model_summaries  \\\n",
       "0  Then, you'll have to go to the hospital.   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  Then, the man asks the woman to tell him the n...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues=dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries=dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries=[]\n",
    "instruct_model_summaries=[]\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt=f\"\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "Summary:\"\"\"\n",
    "    input_ids=tokenizer(prompt,return_tensors='pt').input_ids\n",
    "    original_model_outputs=original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output=tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_outputs=instruct_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output=tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "\n",
    "zipped_summaries=list(zip(human_baseline_summaries,original_model_summaries,instruct_model_summaries))\n",
    "\n",
    "df=pd.DataFrame(zipped_summaries,columns=['human_baseline_summaries','original_model_summaries','instruct_model_summaries'])\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28820d0c-dd0b-4ec6-a090-2e28fde1f5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2222222222222222, 'rouge2': 0.058823529411764705, 'rougeL': 0.16666666666666666, 'rougeLsum': 0.16666666666666666}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.125, 'rouge2': 0.0, 'rougeL': 0.125, 'rougeLsum': 0.125}\n"
     ]
    }
   ],
   "source": [
    "original_model_results=rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "instruct_model_results=rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "051d9c1a-a355-42b8-bb2a-dfc20dca46b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\n",
      "rouge1:-9.72%\n",
      "rouge2:-5.88%\n",
      "rougeL:-4.17%\n",
      "rougeLsum:-4.17%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n",
    "improvement=(np.array(list(instruct_model_results.values())) -np.array(list(original_model_results.values())))\n",
    "for key,value in zip(instruct_model_results.keys(),improvement):\n",
    "             print(f'{key}:{value*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c701a78-b33a-45ec-b3fc-ca943e11e132",
   "metadata": {},
   "source": [
    "# PERFORM PARAMETER EFFICIENT FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "838bc476-2c0a-45a5-bee9-85f10873f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig,get_peft_model,TaskType\n",
    "lora_config=LoraConfig(\n",
    "    r=32,#Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\",\"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98853c6f-3458-4603-98a7-538dabca6a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_model_parameters:3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters:1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model=get_peft_model(original_model,lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047c860-df6d-4874-9e91-f186cf03f670",
   "metadata": {},
   "source": [
    "# Train PEFT Adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "662f486d-2eb4-4c6e-8abe-aa51d6b88eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args=TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "peft_trainer=Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc7848-8613-47aa-9dbe-59749feb4191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhagyashreetikhe52\u001b[0m (\u001b[33martificial_intelligence\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sagemaker-user/wandb/run-20240128_131334-60i4zvhu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/artificial_intelligence/huggingface/runs/60i4zvhu' target=\"_blank\">playful-cloud-4</a></strong> to <a href='https://wandb.ai/artificial_intelligence/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/artificial_intelligence/huggingface' target=\"_blank\">https://wandb.ai/artificial_intelligence/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/artificial_intelligence/huggingface/runs/60i4zvhu' target=\"_blank\">https://wandb.ai/artificial_intelligence/huggingface/runs/60i4zvhu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7bbfe4-a6b4-4640-a2fb-4354da833880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
    " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
    " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec6acac-1f8e-41ef-a481-d5d1d3cada3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_config.json to peft-dialogue-summary-checkpoint-from-s3/adapter_config.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer_config.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer_config.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/special_tokens_map.json to peft-dialogue-summary-checkpoint-from-s3/special_tokens_map.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/tokenizer.json to peft-dialogue-summary-checkpoint-from-s3/tokenizer.json\n",
      "download: s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/adapter_model.bin to peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2c52ad-0777-4207-86c9-5a7f6b78cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 sagemaker-user users 14208525 May 15  2023 ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e4af12-094a-4428-8898-25b165e501d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel,PeftConfig\n",
    "\n",
    "peft_model_base=AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\",torch_dtype=torch.bfloat16)\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model=PeftModel.from_pretrained(peft_model_base,'./peft-dialogue-summary-checkpoint-from-s3/',torch_dtype=torch.bfloat16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b171dcd5-3a1b-4402-904e-1f4610fa7b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_model_parameters:0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters:0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1d358-e252-482e-bc35-a4a066b3c201",
   "metadata": {},
   "source": [
    "# Evaluate the model Qualitatively(human evaluatiion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6f8e3da-a2f0-41e2-9381-80d77fbdc0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1#: I'm thinking of upgrading my computer. #Person2#: I'm not sure what exactly I would need. #Person1#: I'd probably need a painting program. #Person2#: I'd probably need a faster processor, more memory and a faster modem. #Person1#: I'd probably need a CD-ROM drive too.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1# suggests #Person2# adding a painting program to #Person2#'s software and upgrading the hardware. #Person2# also wants to add a CD-ROM drive.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:#Person1#: I'm thinking of upgrading my computer. #Person2#: I'm not sure what exactly I would need. #Person1#: I'd probably need a painting program. #Person2#: I'd probably need a faster processor, more memory and a faster modem. #Person1#: I'd probably need a CD-ROM drive too.\n"
     ]
    }
   ],
   "source": [
    "index=200\n",
    "dialogue=dataset['test'][index]['dialogue']\n",
    "baseline_human_summary=dataset['test'][index]['summary']\n",
    "prompt=f\"\"\"\"\n",
    "Sumarize the following conversation.\n",
    "{dialogue}\n",
    "Summary:\"\"\"\n",
    "input_ids=tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs=original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "original_model_text_output=tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "instruct_model_outputs=instruct_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "instruct_model_text_output=tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "peft_model_outputs=original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "peft_model_text_output=tokenizer.decode(peft_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:{peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d2f7e-6e0a-493d-9b96-26935ffd0df3",
   "metadata": {},
   "source": [
    "# Evaluate the Model Quantitatively(with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8b18945-0a43-4ea5-9cb8-d07cee1bb513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>The memo will go out to all employees by this ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: Happy birthday, Brian. #Person2#: T...</td>\n",
       "      <td>Brian's birthday is coming. #Person1# invites ...</td>\n",
       "      <td>Brian remembers his birthday and invites #Pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  The memo will go out to all employees by this ...   \n",
       "1  The memo will go out to all employees by this ...   \n",
       "2  The memo will go out to all employees by this ...   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "5  The traffic jam at the Carrefour intersection ...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9  #Person1#: Happy birthday, Brian. #Person2#: T...   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "3  #Person2# got stuck in traffic again. #Person1...   \n",
       "4  #Person2# got stuck in traffic again. #Person1...   \n",
       "5  #Person2# got stuck in traffic again. #Person1...   \n",
       "6  Masha and Hero are getting divorced. Kate can'...   \n",
       "7  Masha and Hero are getting divorced. Kate can'...   \n",
       "8  Masha and Hero are getting divorced. Kate can'...   \n",
       "9  Brian's birthday is coming. #Person1# invites ...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic and #Person1# s...  \n",
       "4  #Person2# got stuck in traffic and #Person1# s...  \n",
       "5  #Person2# got stuck in traffic and #Person1# s...  \n",
       "6  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "7  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "8  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "9  Brian remembers his birthday and invites #Pers...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues=dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries=dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries=[]\n",
    "instruct_model_summaries=[]\n",
    "peft_model_summaries=[]\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt=f\"\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "Summary:\"\"\"\n",
    "    input_ids=tokenizer(prompt,return_tensors='pt').input_ids\n",
    "    human_baseline_text_output=human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs=original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output=tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "   \n",
    "    instruct_model_outputs=instruct_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output=tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs=peft_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output=tokenizer.decode(peft_model_outputs[0],skip_special_tokens=True)\n",
    "    \n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries=list(zip(human_baseline_summaries,original_model_summaries,instruct_model_summaries,peft_model_summaries))\n",
    "\n",
    "df=pd.DataFrame(zipped_summaries,columns=['human_baseline_summaries','original_model_summaries','instruct_model_summaries','peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f9ca6fb-a6a4-484e-b3d7-9696e0fea656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.25501323025942, 'rouge2': 0.1087536231884058, 'rougeL': 0.2192207754075008, 'rougeLsum': 0.2223162801267457}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.401144059640105, 'rouge2': 0.17363992702350184, 'rougeL': 0.28844048774453657, 'rougeLsum': 0.28806862350776813}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.3710424494038841, 'rouge2': 0.12029056326962076, 'rougeL': 0.27530950816773303, 'rougeLsum': 0.27654918093340153}\n"
     ]
    }
   ],
   "source": [
    "rouge=evaluate.load('rouge')\n",
    "original_model_results=rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "instruct_model_results=rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "peft_model_results=rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4841ba20-9146-4609-9ec4-203cd6d8e4de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m human_baseline_summaries\u001b[38;5;241m=\u001b[39m\u001b[43mresults\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_baseline_summaries\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      2\u001b[0m original_baseline_summaries\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_baseline_summaries\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m instruct_baseline_summaries\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_baseline_summaries\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "human_baseline_summaries=results['human_baseline_summaries'].values\n",
    "original_baseline_summaries=results['human_baseline_summaries'].values\n",
    "instruct_baseline_summaries=results['human_baseline_summaries'].values\n",
    "peft_baseline_summaries=results['human_baseline_summaries'].values\n",
    "\n",
    "original_model_results=rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "instruct_model_results=rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "peft_model_results=rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99548c6e-f4b2-49cf-aa3f-f8f11aa3b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1153376",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"Yukang/Llama-2-7b-longlora-100k-ft\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"Yukang/Llama-2-7b-longlora-100k-ft\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    \n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\",\"value\"],\n",
    "    r=8,\n",
    "    bias=\"lora_only\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"decode_head\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f935bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(base_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24d3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e491746",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=X_train,\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    "    eval_dataset=X_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "fine_tuning.train()\n",
    "\n",
    "fine_tuning.model.save_pretrained(refined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236abc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
